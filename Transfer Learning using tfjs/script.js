const STATUS = document.getElementById("status")
const VIDEO = document.getElementById("webcam")
const ENABLE_CAM_BUTTON = document.getElementById('enableCam');
const RESET_BUTTON = document.getElementById('reset');
const TRAIN_BUTTON = document.getElementById('train');
const MOBILE_NET_INPUT_WIDTH = 224;
const MOBILE_NET_INPUT_HEIGHT = 224;
const STOP_DATA_GATHER = -1; // This stores a state value so you know when the user has stopped clicking a button to gather data from the webcam feed.
const CLASS_NAMES = [];  // acts as a lookup and holds the human readable names for the possible class predictions. 
const dataCollectorButtons = document.querySelectorAll('button.dataCollector')

// some variables
let mobilenet = undefined;      // store the loaded mobilenet model.
let gatherDataState = STOP_DATA_GATHER;  // If a ‘dataCollector' button is pressed, this changes to be the 1 hot ID of that button instead, as defined in the HTML
let videoPlaying = false;   // keeps track of whether the webcam stream is successfully loaded and playing and is available to use.
let trainingDataInputs = [];  //  store the gathered training data values, as you click the ‘dataCollector' buttons for the input features generated by MobileNet base model, and the output class sampled respectively.
let trainingDataOutputs = [];
let examplesCount = []; // keep track of how many examples are contained for each class once you start adding them.
let predict = false;    // controls your prediction loop.

ENABLE_CAM_BUTTON.addEventListener('click', enableCam);
TRAIN_BUTTON.addEventListener('click', trainAndPredict);
RESET_BUTTON.addEventListener('click', reset)

for (let i=0; i<dataCollectorButtons.length; i++) {
    dataCollectorButtons[i].addEventListener('mousedown', gatherDataForClass)
    dataCollectorButtons[i].addEventListener('mouseup', gatherDataForClass)
    // Populate the human readable names for classes.
    CLASS_NAMES.push(dataCollectorButtons[i].getAttribute('data-name'));
}


// hasGetUserMedia function checks if the "navigator.mediaDevices.getUserMedia" method is available in the browser. This method is typically used for accessing a user's camera or microphone through the WebRTC API.
// The function returns a boolean value, true if navigator.mediaDevices.getUserMedia is supported, and false otherwise. It uses the double negation (!!) to convert the result into a boolean value.
function hasGetUserMedia() {
    // Check if navigator.mediaDevices and navigator.mediaDevices.getUserMedia exist
    // and convert the result to a boolean value
    return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
  }
  
function enableCam() {
    if (hasGetUserMedia()) {
        // getUsermedia parameters.
        const constraints = {
            video: true,
            width: 640, 
            height: 480
        };

        // Activate the webcam stream.
        navigator.mediaDevices.getUserMedia(constraints).then(function(stream) {
            VIDEO.srcObject = stream;
            VIDEO.addEventListener('loadeddata', function () {
                videoPlaying = true;
                ENABLE_CAM_BUTTON.classList.add('revomed');  
            });
        })
    } else {
        console.warn('getUserMedia() is not supported by your browser');
    }

}




// main prediction loop that samples frames from a webcam and continuously predicts what is in each frame with real time results in the browser.
function predictLoop() {
    if (predict) {
      tf.tidy(function() {
        let videoFrameAsTensor = tf.browser.fromPixels(VIDEO).div(255);   // get the image features for the current image just like you did in the dataGatherLoop() function. Essentially, you grab a frame from the webcam using tf.browser.from pixels(), normalise it, resize it to be 224 by 224 pixels in size, and then pass that data through the MobileNet model to get the resulting image features.
        let resizedTensorFrame = tf.image.resizeBilinear(videoFrameAsTensor,[MOBILE_NET_INPUT_HEIGHT, 
            MOBILE_NET_INPUT_WIDTH], true);     
  
        let imageFeatures = mobilenet.predict(resizedTensorFrame.expandDims());  // use your newly trained model head to actually perform a prediction by passing the resulting imageFeatures just found through the trained model's predict()function. You can then squeeze the resulting tensor to make it 1 dimensional again and assign it to a variable called prediction.
        let prediction = model.predict(imageFeatures).squeeze();
        let highestIndex = prediction.argMax().arraySync();   // With this prediction you can find the index that has the highest value using argMax(), and then convert this resulting tensor to an array using arraySync() to get at the underlying data in JavaScript to discover the position of the highest valued element. This value is stored in the variable called highestIndex.
        let predictionArray = prediction.arraySync();   //  get the actual prediction confidence scores in the same way by calling arraySync() on the prediction tensor directly.
  
        STATUS.innerText = 'Prediction: ' + CLASS_NAMES[highestIndex] + ' with ' + Math.floor(predictionArray[highestIndex] * 100) + '% confidence';   // To get the human readable string for the class you can just look up the highestIndex in the CLASS_NAMES array, and then grab the confidence value from the predictionArray. To make it more readable as a percentage, just multiply by 100 and math.floor() the result.
      });
  
      window.requestAnimationFrame(predictLoop);   // Finally, you can use window.requestAnimationFrame() to call predictionLoop() all over again once ready, to get real time classification on your video stream. This continues until predict is set to false if you choose to train a new model with new data.
    }
  }

async function trainAndPredict() {
    predict = false;   // ensure you stop any current predictions from taking place
    tf.util.shuffleCombo(trainingDataInputs, trainingDataOutputs);   // shuffle your input and output arrays using tf.util.shuffleCombo() to ensure the order does not cause issues in training.
    let outputsAsTensor = tf.tensor1d(trainingDataOutputs, 'int32');   // Convert your output array, trainingDataOutputs, to be a tensor1d of type int32 so it is ready to be used in a one hot encoding.
    let oneHotOutputs = tf.oneHot(outputsAsTensor, CLASS_NAMES.length);   // Use the tf.oneHot() function with this outputsAsTensor variable along with the max number of classes to encode, which is just the CLASS_NAMES.length. Your one hot encoded outputs are now stored in a new tensor called oneHotOutputs.
    let inputsAsTensor = tf.stack(trainingDataInputs);  // Note that currently trainingDataInputs is an array of recorded tensors. In order to use these for training you will need to convert the array of tensors to become a regular 2D tensor. To do that there is a great function within the TensorFlow.js library called tf.stack(),
    
    let results = await model.fit(inputsAsTensor, oneHotOutputs, {shuffle: true, batchSize: 5, epochs: 10, 
        callbacks: {onEpochEnd: logProgress} });   // Finally, you can dispose of the created tensors as the model is now trained. You can then set predict back to true to allow predictions to take place again, and then call the predictLoop() function to start predicting live webcam images.
    
    outputsAsTensor.dispose();
    oneHotOutputs.dispose();
    inputsAsTensor.dispose();
    predict = true;
    predictLoop();       // model is above to make predictions.
  }
  
  function logProgress(epoch, logs) {
    console.log('Data for epoch ' + epoch, logs);
  }




/**
 * Purge data and start over. Note this does not dispose of the loaded 
 * MobileNet model and MLP head tensors as you will need to reuse 
 * them to train a new model.
 **/
function reset() {
    // First, stop any running prediction loops by setting predict to false. Next, delete all contents in the examplesCount array by setting its length to 0, which is a handy way to clear all contents from an array.
    predict = false;
    examplesCount.length = 0;

    // Now go through all the current recorded trainingDataInputs and ensure you dispose() of each tensor contained within it to free up memory again, as Tensors are not cleaned up by the JavaScript garbage collector.
    for (let i = 0; i < trainingDataInputs.length; i++) {
      trainingDataInputs[i].dispose();
    }

    // Once that is done you can now safely set the array length to 0 on both the trainingDataInputs and trainingDataOutputs arrays to clear those too.
    trainingDataInputs.length = 0;
    trainingDataOutputs.length = 0;

    // Note: If you had set the length to zero on the "trainingDataInputs" array before disposing the tensors, the tensors would be unreachable but still in memory and not disposed of which would cause a memory leak over time.

    // Finally set the STATUS text to something sensible, and print out the tensors left in memory as a sanity check.
    STATUS.innerText = 'No data collected';
    
    console.log('Tensors in memory: ' + tf.memory().numTensors);
  }




// #################### Data collection ############################
// This function is responsible for sampling images from the webcam video, passing them through the MobileNet model, and capturing the outputs of that model (the 1024 feature vectors).
function dataGatherLoop() {
    if (videoPlaying && gatherDataState !== STOP_DATA_GATHER) {
      let imageFeatures = tf.tidy(function() {      // This function is responsible for sampling images from the webcam video, passing them through the MobileNet model, and capturing the outputs of that model (the 1024 feature vectors).
        let videoFrameAsTensor = tf.browser.fromPixels(VIDEO);   // grab a frame of the webcam VIDEO using tf.browser.fromPixels()
        let resizedTensorFrame = tf.image.resizeBilinear(videoFrameAsTensor, [MOBILE_NET_INPUT_HEIGHT,   
            MOBILE_NET_INPUT_WIDTH], true);   // resize the "videoFrameAsTensor" variable to be of the correct shape for the MobileNet model's input. Use a "tf.image.resizeBilinear()"" call with the tensor you want to reshape as the first parameter, and then a shape that defines the new height and width as defined by the constants you already created earlier. Finally, set align corners to true by passing the third parameter to avoid any alignment issues when resizing.
        let normalizedTensorFrame = resizedTensorFrame.div(255);  // normalize the image data. Image data is always in the range of 0 to 255 when using tf.browser.frompixels(), so you can simply divide resizedTensorFrame by 255 to ensure all values are between 0 and 1 instead, which is what the MobileNet model expects as inputs.
        return mobilenet.predict(normalizedTensorFrame.expandDims()).squeeze();  // Finally, in the tf.tidy() section of the code, push this normalized tensor through the loaded model by calling mobilenet.predict(), to which you pass the expanded version of the normalizedTensorFrame using expandDims() so that it is a batch of 1, as the model expects a batch of inputs for processing.Once the result comes back, you can then immediately call squeeze() on that returned result to squash it back down to a 1D tensor, which you then return and assign to the imageFeatures variable that captures the result from tf.tidy()
      });
  
      trainingDataInputs.push(imageFeatures);   // imageFeatures from the MobileNet model, record those by pushing them onto the trainingDataInputs array that you defined previously.
      trainingDataOutputs.push(gatherDataState);  // record what this input represents by pushing the current gatherDataState to the trainingDataOutputs array too.
       
      // Intialize array index element if currently undefined.
      // At this point you can also increment the number of examples you have for a given class. To do this, first check if the index within the examplesCount array has been initialized before or not. If it is undefined, set it to 0 to initialize the counter for a given class's numerical ID, and then you can increment the examplesCount for the current gatherDataState.
      if (examplesCount[gatherDataState] === undefined) {
        examplesCount[gatherDataState] = 0;
      }
      examplesCount[gatherDataState]++;
  
      // Now update the STATUS element's text on the web page to show the current counts for each class as they're captured. To do this, loop through the CLASS_NAMES array, and print the human readable name combined with the data count at the same index in examplesCount.
      STATUS.innerText = '';
      for (let n = 0; n < CLASS_NAMES.length; n++) {
        STATUS.innerText += CLASS_NAMES[n] + ' data count: ' + examplesCount[n] + '. ';
      }
      window.requestAnimationFrame(dataGatherLoop);    // Finally, call window.requestAnimationFrame() with dataGatherLoop passed as a parameter, to recursively call this function again. This will continue to sample frames from the video until the button's mouseup is detected, and gatherDataState is set to STOP_DATA_GATHER, at which point the data gather loop will end.
    }
  }

/**
 * Handle Data Gather for button mouseup/mousedown.
 **/
function gatherDataForClass() {
    let classNumber = parseInt(this.getAttribute('data-1hot'));
    gatherDataState = (gatherDataState === STOP_DATA_GATHER) ? classNumber : STOP_DATA_GATHER;   //  If the current gatherDataState is equal to STOP_DATA_GATHER (which you set to be -1), then that means you are not currently gathering any data and it was a mousedown event that fired. Set the gatherDataState to be the classNumber you just found. Otherwise, it means that you are currently gathering data and the event that fired was a mouseup event, and you now want to stop gathering data for that class. Just set it back to the STOP_DATA_GATHER state to end the data gathering loop you will define shortly.
    dataGatherLoop();
  }


/**
 * ###################### 1. Loads the MobileNet model and warms it up so ready for use. ############################
 * tf.zeros() wrapped in a tf.tidy() to ensure tensors are disposed of correctly, with a batch size of 1, and the correct height and width that you defined in your constants at the start. Finally, you also specify the color channels, which in this case is 3 as the model expects RGB images.
 **/
async function loadMobileNetFeatureModel() {
    const URL = 'https://tfhub.dev/google/tfjs-model/imagenet/mobilenet_v3_small_100_224/feature_vector/5/default/1';
    
    mobilenet = await tf.loadGraphModel(URL, {fromTFHub: true});
    STATUS.innerText = 'MobileNet v3 loaded successfully!';
    
    // Warm up the model by passing zeros through it once.
    tf.tidy(function () {
    let answer = mobilenet.predict(tf.zeros([1, MOBILE_NET_INPUT_HEIGHT, MOBILE_NET_INPUT_WIDTH, 3]));
    console.log(answer.shape);
    });
}

// Call the function immediately to start loading.
loadMobileNetFeatureModel();


/**
 * ############################### 2. Define the new model head ##############################
 **/

let model = tf.sequential();
model.add(tf.layers.dense({inputShape: [1024], units: 128, activation: 'relu'}));       // This layer has 128 neurons that use the ReLU activation function (1 if x>0 otherwise 0).
model.add(tf.layers.dense({units: CLASS_NAMES.length, activation: 'softmax'}));         // number of neurons should equal the number of classes you are trying to predict which is equal to the number of data gather buttons found in the user interface. softmax activation is often used in the output layer, especially when dealing with categorical variables and multi-class classification problems.

model.summary();  // print the overview of the newly defined model to the console.

// Compile the model with the defined optimizer and specify a loss function to use.
model.compile({

  // Adam changes the learning rate over time which is useful.
  optimizer: 'adam',

  // Use the correct loss function. If 2 classes of data, must use binaryCrossentropy.
  // Else categoricalCrossentropy is used if more than 2 classes.
  loss: (CLASS_NAMES.length === 2) ? 'binaryCrossentropy': 'categoricalCrossentropy',

  // As this is a classification problem you can record accuracy in the logs too!
  metrics: ['accuracy']  // Accuracy metrics monitored in the logs later for debugging purposes.
});

// NOTE:  try changing the number of neurons in the first layer to see how low you can make it while still getting decent performance. Often with machine learning there is some level of trial and error to find optimal parameter values to give you the best trade off between resource usage and speed.





