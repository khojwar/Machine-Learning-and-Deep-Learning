{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For more code resources: https://github.com/tensorflow/tpu/tree/master/tools/colab"
      ],
      "metadata": {
        "id": "hk6tJdx9aXGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objectives\n",
        "\n",
        "\n",
        "\n",
        "*   Build a two-layer, forward-LSTM model.\n",
        "*   Use distribution strategy to produce a `tf.keras` model that runs on TPU version and then use the standard Keras methods to train: `fit`, `predict`, and `evaluate`.\n",
        "* Use the trained model to make predictions and generate your own Shakespeare-esque play.\n",
        "\n"
      ],
      "metadata": {
        "id": "HKTh-Niro1td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Download data\n",
        "\n",
        "Download The Complete Works of William Shakespeare as a single text file from [Project Gutenberg]()."
      ],
      "metadata": {
        "id": "WQ4exVtYpf8m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES2-uQqPohWP",
        "outputId": "e97a758f-249d-4cfb-e190-9c344ac04f64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-05 08:37:57--  http://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/100/100-0.txt [following]\n",
            "--2023-08-05 08:37:57--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5657857 (5.4M) [text/plain]\n",
            "Saving to: ‘/content/shakespeare.txt’\n",
            "\n",
            "/content/shakespear 100%[===================>]   5.40M  18.0MB/s    in 0.3s    \n",
            "\n",
            "2023-08-05 08:37:58 (18.0 MB/s) - ‘/content/shakespeare.txt’ saved [5657857/5657857]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#  \"wget\" download a text file from the specified URL and save it as \"shakespeare.txt\" in the \"/content/\" directory.\n",
        "!wget --show-progress --continue -O /content/shakespeare.txt http://www.gutenberg.org/files/100/100-0.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Build the input dataset"
      ],
      "metadata": {
        "id": "7fdy_rXCiMD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n5 /content/shakespeare.txt    # \"head\" display the beginning (head) of a file. And \"-n5\" display the first 5 lines.\n",
        "!echo \"...\"     # prints three dots \"...\" as a separator\n",
        "!shuf -n5 /content/shakespeare.txt    # The \"shuf\" command is used to shuffle or randomize lines in a file. The \"-n5\" option specifies that it should select 5 lines randomly."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcCtwtiIpqn6",
        "outputId": "907755cc-46a0-4339-ce57-3062b65ffeb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "...\n",
            "Call up my brother. O, would you had had her!\n",
            "Since Henry’s death, I fear, there is conveyance.\n",
            "\n",
            "May it be possible that foreign hire\n",
            "Puts bars between the owners and their rights!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " This code is creating a TensorFlow dataset for training a language model using Shakespeare's text. It reads the text from a file, converts it into an array of integers, splits the text into input and target sequences, shuffles and batches the sequences, and finally, repeats the dataset to enable multiple epochs of training."
      ],
      "metadata": {
        "id": "AjHPQOPvnjfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ord(c)` returns the ASCII value of a single character c. For example, ord('a') would return 97.\n",
        "\n",
        "`tf.io.gfile.GFile` allows TensorFlow to handle a wide range of storage systems, such as local files, Google Cloud Storage, HDFS, Amazon S3, etc., without needing to change your code.\n",
        "\n",
        "you can also use Python's built-in `open` function for file I/O. However, using `tf.io.gfile.GFile` is recommended when working within TensorFlow environments to maintain compatibility across different storage systems."
      ],
      "metadata": {
        "id": "56gEJd_sgYgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Check if the TensorFlow version is compatible with this notebook\n",
        "import distutils\n",
        "if distutils.version.LooseVersion(tf.__version__) < '2.0':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 2.0 or higher.')\n",
        "\n",
        "# Define the path to the input text file containing Shakespeare's text\n",
        "SHAKESPEARE_TXT = '/content/shakespeare.txt'\n",
        "\n",
        "# Function to transform a string of text into an array of integer values\n",
        "def transform(txt):\n",
        "    return np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
        "\n",
        "# Function to create a TensorFlow dataset for training sequences\n",
        "def input_fn(seq_len=100, batch_size=1024):\n",
        "    \"\"\"Return a dataset of source and target sequences for training.\"\"\"\n",
        "\n",
        "    # Read the content of the input text file\n",
        "    with tf.io.gfile.GFile(SHAKESPEARE_TXT, 'r') as f:\n",
        "        txt = f.read()\n",
        "\n",
        "    # Convert the text into a sequence of integer values using the transform function\n",
        "    source = tf.constant(transform(txt), dtype=tf.int32)\n",
        "\n",
        "    # Create a TensorFlow dataset with sequences of length 'seq_len' and drop any remainder\n",
        "    ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1, drop_remainder=True)       # It's a common approach to prepare data for training RNNs\n",
        "\n",
        "    # Define a function to split input and target sequences for each chunk in the dataset\n",
        "    def split_input_target(chunk):\n",
        "        input_text = chunk[:-1]\n",
        "        target_text = chunk[1:]\n",
        "        return input_text, target_text\n",
        "\n",
        "    # Set the buffer size for shuffling the dataset\n",
        "    BUFFER_SIZE = 10000\n",
        "\n",
        "    # Apply the split_input_target function to the dataset, shuffle it, and batch it\n",
        "    ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    # Repeat the dataset indefinitely to allow multiple epochs during training\n",
        "    return ds.repeat()\n"
      ],
      "metadata": {
        "id": "f4DmvjHGqqgb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f89fa82-51dd-4302-8ba4-a5294fd354fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-0769144f0cf2>:8: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if distutils.version.LooseVersion(tf.__version__) < '2.0':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Build the model\n",
        "\n",
        "The model is defined as a two-layer, forward-LSTM, the same model should work both on CPU and TPU.\n",
        "\n",
        "Because our vocabulary size (*number of unique words/tokens present in a given text corpus or dataset.*) is 256, the input dimension to the Embedding layer is 256.\n",
        "\n",
        "(*For example: The vocabulary size for this sentence would be 9 because there are 9 unique words: \"The,\" \"quick,\" \"brown,\" \"fox,\" \"jumps,\" \"over,\" \"the,\" \"lazy,\" and \"dog.\"*)\n",
        "\n",
        "When specifying the arguments to the LSTM, it is important to note how the stateful argument is used. When training we will make sure that `stateful=False` because we do want to reset the state of our model between batches, but when sampling (computing predictions) from a trained model, we want `stateful=True` so that the model can retain information across the current batch and generate more interesting text."
      ],
      "metadata": {
        "id": "nyvo5iZbiVkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the embedding dimension\n",
        "EMBEDDING_DIM = 512\n",
        "\n",
        "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
        "    \"\"\"\n",
        "    Language model: predict the next word given the current word.\n",
        "\n",
        "    Args:\n",
        "    seq_len (int): Length of the input sequence.\n",
        "    batch_size (int): Batch size for training the model.\n",
        "    stateful (bool): If True, the LSTM layers will maintain state between batches.\n",
        "\n",
        "    Returns:\n",
        "    tf.keras.Model: A Keras Model representing the LSTM language model.\n",
        "    \"\"\"\n",
        "    # Define the input layer for the model\n",
        "    source = tf.keras.Input(\n",
        "        name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
        "\n",
        "    # Create an embedding layer to convert integer indices to dense vectors\n",
        "    embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
        "\n",
        "    # Create the first LSTM layer with specified embedding dimension and stateful parameter\n",
        "    lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
        "\n",
        "    # Create the second LSTM layer with specified embedding dimension and stateful parameter\n",
        "    lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
        "\n",
        "    # Apply TimeDistributed dense layer with softmax activation for predicting the next character\n",
        "    predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
        "\n",
        "    # Return the Keras model with input and output layers\n",
        "    return tf.keras.Model(inputs=[source], outputs=[predicted_char])\n"
      ],
      "metadata": {
        "id": "HXreWc5aiYwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Train the model\n",
        "\n",
        "First, we need to create a distribution strategy that can use the TPU ( *OR setting up TensorFlow to run on a Google Cloud TPU (Tensor Processing Unit)*). In this case it is TPUStrategy. You can create and compile the model inside its scope. Once that is done, future calls to the standard Keras methods `fit`, `evaluate` and `predict` use the TPU.\n",
        "\n",
        "Again note that we train with `stateful=False` because while training, we only care about one batch at a time."
      ],
      "metadata": {
        "id": "g-e1wDjOuwxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear any existing TensorFlow sessions and reset the Keras backend state to avoid conflicts.\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Initialize the TPUClusterResolver to connect to the Cloud TPU.\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "# Connect to the TPU cluster and set up the TPU system for computation.\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "\n",
        "# Print the list of logical devices (TPU cores) available for use.\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "# Create a TPUStrategy object to enable distributed training on the TPU.\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "# Enter the strategy scope to execute the following operations on the TPU.\n",
        "with strategy.scope():\n",
        "    # Create the LSTM model for training with a sequence length of 100 and stateful set to False.\n",
        "    training_model = lstm_model(seq_len=100, stateful=False)\n",
        "\n",
        "    # Compile the model with the RMSprop optimizer, sparse categorical cross-entropy loss,\n",
        "    # and sparse categorical accuracy as the metric.\n",
        "    training_model.compile(\n",
        "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['sparse_categorical_accuracy']\n",
        "    )\n",
        "\n",
        "# Train the model using the input_fn() to provide training data, with 100 steps per epoch\n",
        "# and for a total of 10 epochs.\n",
        "training_model.fit(\n",
        "    input_fn(),\n",
        "    steps_per_epoch=100,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# Save the trained model weights to '/tmp/bard.h5' in HDF5 format, overwriting if the file exists.\n",
        "training_model.save_weights('/tmp/bard.h5', overwrite=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "eml7JRHM1mvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c2e48b-1544-442c-ed83-b53d7c5da5ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 14s 65ms/step - loss: 3.5829 - sparse_categorical_accuracy: 0.1432\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 3.3783 - sparse_categorical_accuracy: 0.1475\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 3.3631 - sparse_categorical_accuracy: 0.1520\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 3.3567 - sparse_categorical_accuracy: 0.1552\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 7s 69ms/step - loss: 3.3511 - sparse_categorical_accuracy: 0.1567\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 7s 69ms/step - loss: 3.3449 - sparse_categorical_accuracy: 0.1574\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 3.2999 - sparse_categorical_accuracy: 0.1553\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 7s 70ms/step - loss: 2.8226 - sparse_categorical_accuracy: 0.2246\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 7s 65ms/step - loss: 2.4063 - sparse_categorical_accuracy: 0.3036\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 7s 69ms/step - loss: 2.2340 - sparse_categorical_accuracy: 0.3484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Make predictions with the model\n",
        "\n",
        "Use the trained model to make predictions and generate your own Shakespeare-esque play.\n",
        "Start the model off with a *seed* sentence, then generate 250 characters from it. The model makes five predictions from the initial seed.\n",
        "\n",
        "The predictions are done on the CPU so the batch size (5) in this case does not have to be divisible by 8.\n",
        "\n",
        "Note that when we are doing predictions or, to be more precise, text generation, we set `stateful=True` so that the model's state is kept between batches. If stateful is false, the model state is reset between each batch, and the model will only be able to use the information from the current batch (a single character) to make a prediction.\n",
        "\n",
        "The output of the model is a set of probabilities for the next character (given the input so far). To build a paragraph, we predict one character at a time and sample a character (based on the probabilities provided by the model). For example, if the input character is \"o\" and the output probabilities are \"p\" (0.65), \"t\" (0.30), others characters (0.05), then we allow our model to generate text other than just \"Ophelia\" and \"Othello.\""
      ],
      "metadata": {
        "id": "ZFxkF-tPxoRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size and prediction length\n",
        "BATCH_SIZE = 5\n",
        "PREDICT_LEN = 250\n",
        "\n",
        "# Create a prediction model using the defined lstm_model with stateful=True\n",
        "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
        "# Load pre-trained weights into the prediction_model from a file\n",
        "prediction_model.load_weights('/tmp/bard.h5')\n",
        "\n",
        "# Seed the model with the initial string, copied BATCH_SIZE times\n",
        "seed_txt = 'Looks it not like the king? Verily, we must go! '\n",
        "seed = transform(seed_txt)\n",
        "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
        "\n",
        "# First, run the seed forward to prime the state of the model.\n",
        "prediction_model.reset_states()\n",
        "for i in range(len(seed_txt) - 1):\n",
        "    # Predict one character at a time for the seed text to prime the model state\n",
        "    prediction_model.predict(seed[:, i:i + 1])\n",
        "\n",
        "# Now we can accumulate predictions!\n",
        "predictions = [seed[:, -1:]]\n",
        "for i in range(PREDICT_LEN):\n",
        "    last_word = predictions[-1]\n",
        "    next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
        "\n",
        "    # Sample from the output distribution to get the next character index\n",
        "    next_idx = [\n",
        "        np.random.choice(256, p=next_probits[i])\n",
        "        for i in range(BATCH_SIZE)\n",
        "    ]\n",
        "    predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
        "\n",
        "# Print the generated text for each batch in BATCH_SIZE\n",
        "for i in range(BATCH_SIZE):\n",
        "    print('PREDICTION %d\\n\\n' % i)\n",
        "    # Extract the i-th batch of predictions\n",
        "    p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
        "    # Convert the predicted character indices back to text\n",
        "    generated = ''.join([chr(c) for c in p])\n",
        "    print(generated)\n",
        "    print()\n",
        "    # Check if the generated text length matches the desired prediction length\n",
        "    assert len(generated) == PREDICT_LEN, 'Generated text too short'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JgkwmE6ExjbU",
        "outputId": "43cea3a3-d2df-4012-8212-7b9a25908786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 852ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-913b952bdca2>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPREDICT_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlast_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mnext_probits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Sample from the output distribution to get the next character index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/layers/rnn/lstm.py\", line 615, in call\n        timesteps = input_shape[0] if self.time_major else input_shape[1]\n\n    TypeError: Exception encountered when calling layer 'lstm_2' (type LSTM).\n    \n    'NoneType' object is not subscriptable\n    \n    Call arguments received by layer 'lstm_2' (type LSTM):\n      • inputs=tf.Tensor(shape=<unknown>, dtype=float32)\n      • mask=None\n      • training=False\n      • initial_state=None\n"
          ]
        }
      ]
    }
  ]
}
